# main.py
# ì´ë¯¸ì§€ ë¶„ë¥˜ - 5-Fold ì•™ìƒë¸” + ì œì¶œ íŒŒì¼ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import timm
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
import os
from datetime import datetime

# ========== ì„¤ì • ==========
IMG_SIZE = 380
BATCH_SIZE = 16
EPOCHS = 15
LR = 0.0003
N_FOLDS = 5
MODEL_NAME = 'tf_efficientnetv2_m'  # ëª¨ë¸ ì„ íƒ: 'tf_efficientnetv2_s'(ì‘ìŒ), 'tf_efficientnetv2_m'(ì¤‘ê°„), 'efficientnet_b0'(ë§¤ìš° ì‘ìŒ)
DROPOUT_RATE = 0.4  # Dropout ë¹„ìœ¨ (0.0 ~ 1.0) - ë°ì´í„°ê°€ ì ìœ¼ë©´ 0.4~0.5 ê¶Œì¥
PATIENCE = 3  # Early stopping patience (F1 ê°œì„ ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨) - ë°ì´í„°ê°€ ì ìœ¼ë©´ ë” ì§§ê²Œ
WEIGHT_DECAY = 0.01  # L2 ì •ê·œí™” ê°•ë„ (0.001~0.1, ë°ì´í„°ê°€ ì ìœ¼ë©´ ì¦ê°€)
LABEL_SMOOTHING = 0.1  # Label Smoothing (0.0 = ì‚¬ìš©ì•ˆí•¨, 0.1 = ê¶Œì¥)
USE_MIXUP = True  # MixUp augmentation ì‚¬ìš© ì—¬ë¶€
MIXUP_ALPHA = 0.2  # MixUp alpha íŒŒë¼ë¯¸í„° (ì‘ì„ìˆ˜ë¡ ë” ê°•í•¨, 0.1~0.4 ê¶Œì¥)
USE_CUTMIX = False  # CutMix augmentation ì‚¬ìš© ì—¬ë¶€ (MixUpê³¼ ë™ì‹œ ì‚¬ìš© ê°€ëŠ¥)
DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'

# íƒ€ì„ìŠ¤íƒ¬í”„ ë³€ìˆ˜ (ë©”ì¸ ì‹¤í–‰ ì‹œ ì´ˆê¸°í™”ë¨ - ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì „)
TIMESTAMP = None

# ========== ë°ì´í„°ì…‹ ==========
class MyDataset(Dataset):
    def __init__(self, df, transform, is_test=False):
        self.df = df.reset_index(drop=True)
        self.transform = transform
        self.is_test = is_test
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(row['image_path'])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ì¦ê°• ì ìš©
        image = self.transform(image=image)['image']
        
        if self.is_test:
            return image
        else:
            label = row['label']
            return image, label

# ========== ì¦ê°• ==========
train_transform = A.Compose([
    A.Resize(IMG_SIZE, IMG_SIZE),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.3),
    A.Rotate(limit=15, p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.Affine(translate_percent=0.1, scale=(0.9, 1.1), rotate=10, p=0.5),  # ì´ë™, í™•ëŒ€/ì¶•ì†Œ, íšŒì „
    A.OneOf([
        A.GaussianBlur(blur_limit=(3, 7), p=1.0),
        A.MedianBlur(blur_limit=5, p=1.0),
        A.MotionBlur(blur_limit=7, p=1.0),
    ], p=0.3),  # ë¸”ëŸ¬ íš¨ê³¼
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

val_transform = A.Compose([
    A.Resize(IMG_SIZE, IMG_SIZE),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# ========== MixUp í•¨ìˆ˜ ==========
def mixup_data(x, y, alpha=1.0):
    """MixUp augmentation"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(DEVICE)
    
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """MixUp loss ê³„ì‚°"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# ========== í•™ìŠµ í•¨ìˆ˜ ==========
def train_epoch(model, loader, criterion, optimizer, scheduler, use_mixup=False, mixup_alpha=0.2):
    model.train()
    losses = []
    
    for images, labels in tqdm(loader, desc='Train'):
        images = images.to(DEVICE)
        labels = labels.to(DEVICE)
        
        # MixUp ì ìš©
        if use_mixup and np.random.random() > 0.5:  # 50% í™•ë¥ ë¡œ MixUp ì ìš©
            images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha)
            optimizer.zero_grad()
            outputs = model(images)
            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
        else:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
        
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        losses.append(loss.item())
    
    return np.mean(losses)

def validate(model, loader):
    model.eval()
    preds_list = []
    labels_list = []
    
    with torch.no_grad():
        for images, labels in tqdm(loader, desc='Val'):
            images = images.to(DEVICE)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            
            preds_list.extend(preds.cpu().numpy())
            labels_list.extend(labels.numpy())
    
    f1 = f1_score(labels_list, preds_list, average='macro')
    return f1

# ========== í´ë“œ í•™ìŠµ ==========
def train_fold(fold, train_df, val_df, exp_dir):
    print(f'\n{"="*50}')
    print(f'Fold {fold} í•™ìŠµ ì‹œì‘')
    print(f'{"="*50}')
    
    # ë°ì´í„° ë¡œë”
    train_dataset = MyDataset(train_df, train_transform)
    val_dataset = MyDataset(val_df, val_transform)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    
    # ëª¨ë¸
    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=17, drop_rate=DROPOUT_RATE)
    model = model.to(DEVICE)
    
    # ì˜µí‹°ë§ˆì´ì € & ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=LR,
        epochs=EPOCHS,
        steps_per_epoch=len(train_loader)
    )
    # Label Smoothing ì ìš©
    if LABEL_SMOOTHING > 0:
        criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)
    else:
        criterion = nn.CrossEntropyLoss()
    
    # í•™ìŠµ ë£¨í”„
    best_f1 = 0
    best_model_state = None  # ë² ìŠ¤íŠ¸ ëª¨ë¸ ìƒíƒœ ì €ì¥
    patience_counter = 0
    patience = PATIENCE
    
    for epoch in range(EPOCHS):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, 
                                 use_mixup=USE_MIXUP, mixup_alpha=MIXUP_ALPHA)
        val_f1 = validate(model, val_loader)
        
        print(f'Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f}, F1: {val_f1:.4f}')
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì—…ë°ì´íŠ¸
        if val_f1 > best_f1:
            best_f1 = val_f1
            best_model_state = model.state_dict().copy()  # ë² ìŠ¤íŠ¸ ëª¨ë¸ ìƒíƒœ ì €ì¥
            patience_counter = 0
            print(f'âœ… Best F1 ì—…ë°ì´íŠ¸: {best_f1:.4f}')
        else:
            patience_counter += 1
            print(f'â³ Patience: {patience_counter}/{patience}')
        
        # Early Stopping
        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch+1}')
            break
    
    # í´ë“œ í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (ê° í´ë“œë‹¹ í•˜ë‚˜ì˜ íŒŒì¼ë§Œ)
    model_filename = f'{exp_dir}/models/fold{fold}_{TIMESTAMP}_f1{best_f1:.4f}.pth'
    torch.save(best_model_state, model_filename)
    print(f'\nFold {fold} ì™„ë£Œ - Best F1: {best_f1:.4f} - ì €ì¥: {model_filename}')
    
    return best_f1

# ========== TTA ì˜ˆì¸¡ ==========
def predict_with_tta(model, image):
    """TTAë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
    model.eval()
    predictions = []
    
    with torch.no_grad():
        # Original
        pred = model(image)
        predictions.append(pred)
        
        # Horizontal Flip
        pred = model(torch.flip(image, dims=[3]))
        predictions.append(pred)
        
        # Vertical Flip
        pred = model(torch.flip(image, dims=[2]))
        predictions.append(pred)
        
        # Both Flips
        pred = model(torch.flip(image, dims=[2, 3]))
        predictions.append(pred)
    
    # Average
    final_pred = torch.stack(predictions).mean(dim=0)
    return final_pred

# ========== ì•™ìƒë¸” ì¶”ë¡  ==========
def inference_ensemble(test_df, fold_info, use_tta=True):
    """ì—¬ëŸ¬ í´ë“œ ëª¨ë¸ë¡œ ì•™ìƒë¸” ì¶”ë¡ 
    
    Args:
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
        fold_info: [(foldë²ˆí˜¸, f1ì ìˆ˜, íŒŒì¼ê²½ë¡œ), ...] ë¦¬ìŠ¤íŠ¸
        use_tta: TTA ì‚¬ìš© ì—¬ë¶€
    """
    print(f'\n{"="*50}')
    print(f'ì¶”ë¡  ì‹œì‘')
    print(f'ëª¨ë¸ ê°œìˆ˜: {len(fold_info)}')
    print(f'TTA: {use_tta}')
    print(f'{"="*50}')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
    test_dataset = MyDataset(test_df, val_transform, is_test=True)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)
    
    # ëª¨ë¸ ë¡œë“œ
    models = []
    fold_f1s = []
    avg_f1 = 0
    for fold, f1, model_path in fold_info:
        model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=17)
        model.load_state_dict(torch.load(model_path, weights_only=False))
        model = model.to(DEVICE)
        model.eval()
        models.append(model)
        fold_f1s.append(f1)
        avg_f1 += f1
        print(f'âœ… Fold {fold} (F1: {f1:.4f}) ë¡œë“œ')
    
    avg_f1 /= len(fold_info)
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚° (F1 ì ìˆ˜ì— ë¹„ë¡€)
    weights = torch.tensor(fold_f1s, dtype=torch.float32)
    weights = weights / weights.sum()  # ì •ê·œí™”
    print(f'\nğŸ“Š ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {dict(zip([f[0] for f in fold_info], weights.tolist()))}')
    
    # ì¶”ë¡ 
    all_predictions = []
    
    for images in tqdm(test_loader, desc='Inference'):
        images = images.to(DEVICE)
        
        fold_preds = []
        for model in models:
            if use_tta:
                pred = predict_with_tta(model, images)
            else:
                with torch.no_grad():
                    pred = model(images)
            
            fold_preds.append(pred.cpu())
        
        # í´ë“œ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê·  - F1 ì ìˆ˜ ê¸°ë°˜)
        fold_preds_tensor = torch.stack(fold_preds)  # [num_models, batch_size, num_classes]
        weights_expanded = weights.unsqueeze(1).unsqueeze(2)  # [num_models, 1, 1]
        ensemble_pred = (fold_preds_tensor * weights_expanded).sum(dim=0)  # ê°€ì¤‘ í•©
        final_class = ensemble_pred.argmax(dim=1).item()
        all_predictions.append(final_class)
    
    return all_predictions, avg_f1

# ========== ì œì¶œ íŒŒì¼ ìƒì„± ==========
def create_submission(test_df, predictions, avg_f1, exp_dir, filename_prefix='submission'):
    """ì œì¶œ íŒŒì¼ ìƒì„± (ë‚ ì§œ_ì‹œê°„_f1score í¬í•¨)"""
    
    # í–‰ ìˆ˜ ê²€ì¦
    if len(predictions) != len(test_df):
        raise ValueError(
            f'âŒ ì˜ˆì¸¡ ê²°ê³¼ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° í–‰ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\n'
            f'   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}í–‰\n'
            f'   ì˜ˆì¸¡ ê²°ê³¼: {len(predictions)}í–‰'
        )
    
    # íŒŒì¼ëª…: submission_{timestamp}_f1{score}.csv
    filename = f'{exp_dir}/{filename_prefix}_{TIMESTAMP}_f1{avg_f1:.4f}.csv'
    
    submission = pd.DataFrame({
        'ID': test_df['ID'],
        'target': predictions
    })
    
    submission.to_csv(filename, index=False)
    
    print(f'\n{"="*50}')
    print(f'ì œì¶œ íŒŒì¼ ìƒì„±: {filename}')
    print(f'{"="*50}')
    print(submission.head(10))
    print(f'\nì˜ˆì¸¡ ë¶„í¬:')
    print(submission['target'].value_counts().sort_index())
    print(f'\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!')
    
    return filename

# ========== ë©”ì¸ ì‹¤í–‰ ==========
if __name__ == '__main__':
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ - ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì „)
    TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ì‹¤í—˜ í´ë” ìƒì„± (ê° ì‹¤í—˜ë§ˆë‹¤ ë³„ë„ í´ë”)
    EXP_DIR = f'experiments/exp_{TIMESTAMP}'
    os.makedirs(EXP_DIR, exist_ok=True)
    os.makedirs(f'{EXP_DIR}/models', exist_ok=True)
    
    print('='*50)
    print('ì´ë¯¸ì§€ ë¶„ë¥˜ í•™ìŠµ & ì¶”ë¡ ')
    print('='*50)
    print(f'Timestamp: {TIMESTAMP}')
    print(f'Experiment folder: {EXP_DIR}')
    print(f'Using device: {DEVICE}')
    print('='*50)
    
    # ===== 1. í•™ìŠµ ë°ì´í„° ë¡œë“œ =====
    train_df = pd.read_csv('data/train.csv')
    
    # ë°ì´í„° ê²€ì¦
    if len(train_df) == 0:
        raise ValueError('âŒ í•™ìŠµ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!')
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ê°€
    train_df['image_path'] = train_df['ID'].apply(lambda x: f'data/train/{x}')
    # target ì»¬ëŸ¼ì„ labelë¡œ ë³€ê²½ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
    train_df['label'] = train_df['target']
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
    missing_images = train_df[~train_df['image_path'].apply(os.path.exists)]
    if len(missing_images) > 0:
        print(f'âš ï¸  ê²½ê³ : {len(missing_images)}ê°œì˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        print(f'ì²« 5ê°œ: {missing_images["ID"].head().tolist()}')
    
    print(f'í•™ìŠµ ë°ì´í„°: {len(train_df)}ì¥')
    print(f'í´ë˜ìŠ¤: {train_df["label"].nunique()}ê°œ')
    
    # ===== 2. K-Fold í•™ìŠµ =====
    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label']), start=1):
        train_fold_df = train_df.iloc[train_idx]
        val_fold_df = train_df.iloc[val_idx]
        
        best_f1 = train_fold(fold, train_fold_df, val_fold_df, EXP_DIR)
        
        # ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        model_path = f'{EXP_DIR}/models/fold{fold}_{TIMESTAMP}_f1{best_f1:.4f}.pth'
        
        fold_results.append({
            'fold': fold,
            'f1': best_f1,
            'model_path': model_path
        })
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    results_df = pd.DataFrame(fold_results)
    print(f'\n{"="*50}')
    print('í•™ìŠµ ê²°ê³¼')
    print(f'{"="*50}')
    print(results_df[['fold', 'f1']])
    print(f'\ní‰ê·  F1: {results_df["f1"].mean():.4f}')
    print(f'ìµœê³  F1: {results_df["f1"].max():.4f}')
    
    # ê²°ê³¼ CSV ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    results_filename = f'{EXP_DIR}/fold_results_{TIMESTAMP}_avgf1{results_df["f1"].mean():.4f}.csv'
    results_df.to_csv(results_filename, index=False)
    print(f'\nê²°ê³¼ ì €ì¥: {results_filename}')
    
    # ===== 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ =====
    # test.csvê°€ ì—†ìœ¼ë©´ sample_submission.csv ì‚¬ìš©
    if os.path.exists('data/test.csv'):
        test_df = pd.read_csv('data/test.csv')
    else:
        test_df = pd.read_csv('data/sample_submission.csv')
        # target ì»¬ëŸ¼ ì œê±° (ì˜ˆì¸¡í•´ì•¼ í•  ê°’ì´ë¯€ë¡œ)
        test_df = test_df.drop('target', axis=1)
    
    # ë°ì´í„° ê²€ì¦
    if len(test_df) == 0:
        raise ValueError('âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!')
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ê°€
    test_df['image_path'] = test_df['ID'].apply(lambda x: f'data/test/{x}')
    # ID ì»¬ëŸ¼ì„ idë¡œ ë³€ê²½ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±)
    test_df['id'] = test_df['ID']
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
    missing_images = test_df[~test_df['image_path'].apply(os.path.exists)]
    if len(missing_images) > 0:
        print(f'âš ï¸  ê²½ê³ : {len(missing_images)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        print(f'ì²« 5ê°œ: {missing_images["ID"].head().tolist()}')
    
    print(f'\ní…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ì¥')
    
    # ===== 4. ëª¨ë“  í´ë“œ ì„ íƒ (ë˜ëŠ” ìƒìœ„ Nê°œ ì„ íƒ) =====
    # ëª¨ë“  í´ë“œ ì‚¬ìš© (ê³¼ì í•© ë°©ì§€ ë° ë‹¤ì–‘ì„± í™•ë³´)
    # ë°ì´í„°ê°€ ì ì„ ë•ŒëŠ” ëª¨ë“  í´ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë¨
    USE_ALL_FOLDS = True  # True: ëª¨ë“  í´ë“œ ì‚¬ìš©, False: ìƒìœ„ Nê°œë§Œ ì‚¬ìš©
    TOP_N_FOLDS = 4  # USE_ALL_FOLDS=Falseì¼ ë•Œ ì‚¬ìš©í•  ìƒìœ„ í´ë“œ ê°œìˆ˜
    
    if USE_ALL_FOLDS:
        selected_folds = results_df
        print(f'\nâœ… ëª¨ë“  í´ë“œ ì‚¬ìš©: {sorted(selected_folds["fold"].tolist())}')
    else:
        results_df_sorted = results_df.sort_values('f1', ascending=False)
        selected_folds = results_df_sorted.head(TOP_N_FOLDS)
        print(f'\nâœ… ìƒìœ„ {TOP_N_FOLDS}ê°œ í´ë“œ ì„ íƒ: {sorted(selected_folds["fold"].tolist())}')
    
    fold_info = [
        (row['fold'], row['f1'], row['model_path'])
        for _, row in selected_folds.iterrows()
    ]
    
    print(f'ì„ íƒëœ í´ë“œ: {[f[0] for f in fold_info]}')
    
    # ===== 5. ì•™ìƒë¸” ì¶”ë¡  =====
    predictions, avg_f1 = inference_ensemble(test_df, fold_info=fold_info, use_tta=False)
    
    # ===== 6. ì œì¶œ íŒŒì¼ ìƒì„± =====
    submission_filename = create_submission(test_df, predictions, avg_f1, EXP_DIR, filename_prefix='submission')
    
    print(f'\n{"="*50}')
    print('ìƒì„±ëœ íŒŒì¼ë“¤')
    print(f'{"="*50}')
    print(f'ğŸ“ ëª¨ë¸ íŒŒì¼:')
    for fold, f1, path in fold_info:
        print(f'  - {path}')
    print(f'\nğŸ“ ê²°ê³¼ íŒŒì¼: {results_filename}')
    print(f'ğŸ“ ì œì¶œ íŒŒì¼: {submission_filename}')
    
    print('\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!')